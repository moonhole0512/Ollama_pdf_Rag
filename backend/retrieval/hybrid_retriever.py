import re
from typing import List, Tuple, Dict, Any
from difflib import SequenceMatcher
from langchain_core.documents import Document
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.embeddings import Embeddings

# This will be initialized from main.py to avoid circular model loading
cross_encoder = None

def set_cross_encoder_model(model):
    global cross_encoder
    cross_encoder = model

def fuzzy_similarity(s1: str, s2: str) -> float:
    """Fuzzy ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° (0.0 ~ 1.0)"""
    return SequenceMatcher(None, s1.lower(), s2.lower()).ratio()

def preprocess_query(query: str) -> str:
    """ì¿¼ë¦¬ ì „ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°, í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ì¤‘ë³µ ë”°ì˜´í‘œ ì œê±°
    query = re.sub(r'["\']{2,}', '', query)
    query = re.sub(r'["\']', '', query)
    return query.strip()

def calculate_chunk_importance(doc: Document, query: str) -> float:
    """ì²­í¬ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (í–¥ìƒëœ ë²„ì „)"""
    
    content = doc.page_content.lower()
    query_lower = query.lower()
    query_terms = query_lower.split()
    
    # 1. ì •í™•í•œ ì¿¼ë¦¬ ë§¤ì¹­ (ìµœìš°ì„ )
    if query_lower in content:
        exact_match_bonus = 50.0
    else:
        exact_match_bonus = 0.0
    
    # 2. Fuzzy ë§¤ì¹­ (OCR ì˜¤ë¥˜ ëŒ€ì‘)
    max_fuzzy_score = 0.0
    content_chunks = content.split('\n')
    for chunk in content_chunks:
        if len(chunk) > 10:  # ë„ˆë¬´ ì§§ì€ ë¼ì¸ ì œì™¸
            fuzzy_score = fuzzy_similarity(query_lower, chunk)
            if fuzzy_score > max_fuzzy_score:
                max_fuzzy_score = fuzzy_score
    
    fuzzy_bonus = max_fuzzy_score * 30.0 if max_fuzzy_score > 0.6 else 0.0
    
    # 3. í‚¤ì›Œë“œ ë¹ˆë„
    keyword_count = sum(content.count(term) for term in query_terms)
    
    # 4. í‚¤ì›Œë“œ ë°€ë„
    keyword_density = keyword_count / len(content) if len(content) > 0 else 0
    
    # 5. ê°œë… ì œëª© ë§¤ì¹­ (í–¥ìƒ)
    concept_title = doc.metadata.get("concept_title", "").lower()
    title_match_score = 0.0
    
    if concept_title and concept_title != "n/a":
        # ì •í™•í•œ ë§¤ì¹­
        if query_lower in concept_title or concept_title in query_lower:
            title_match_score = 40.0
        else:
            # Fuzzy ë§¤ì¹­
            fuzzy_title_score = fuzzy_similarity(query_lower, concept_title)
            if fuzzy_title_score > 0.5:
                title_match_score = fuzzy_title_score * 30.0
    
    # 6. ì±•í„° ì œëª© ë§¤ì¹­
    chapter_title = doc.metadata.get("chapter_title", "").lower()
    chapter_match_score = 0.0
    
    if chapter_title and chapter_title != "n/a":
        for term in query_terms:
            if term in chapter_title:
                chapter_match_score += 5.0
    
    # 7. í˜ì´ì§€ ìœ„ì¹˜ ë³´ë„ˆìŠ¤ (ê°œë… ì •ì˜ëŠ” ë³´í†µ ì•ë¶€ë¶„)
    position_bonus = 1.0
    page_num = doc.metadata.get("page", 999)
    if page_num < 100:
        position_bonus = 1.1
    elif page_num < 200:
        position_bonus = 1.05
    
    # ì¢…í•© ì ìˆ˜
    importance = (
        exact_match_bonus +
        fuzzy_bonus +
        title_match_score +
        chapter_match_score +
        keyword_density * 100 +
        keyword_count * 2
    ) * position_bonus
    
    return importance

def get_adaptive_k(query_info: Dict[str, Any], query: str) -> Dict[str, int]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ ê²€ìƒ‰ ê¹Šì´ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •"""
    
    # ê¸°ë³¸ê°’
    initial_k = 40
    final_k = 10
    
    query_lower = query.lower()
    
    # 1. íŠ¹ì • ê°œë…/ìš©ì–´ ê²€ìƒ‰ (ì •í™•í•œ ë§¤ì¹­ í•„ìš”)
    concept_indicators = [
        "ì´ë€", "ì´ë¼ëŠ”", "ë¬´ì—‡", "ë­", "ì •ì˜", "ì˜ë¯¸",
        "ì „ëµ", "ì›ë¦¬", "ë²•ì¹™", "íš¨ê³¼", "ë°©ë²•", "ê¸°ë²•"
    ]
    if any(indicator in query_lower for indicator in concept_indicators):
        initial_k = 50
        final_k = 12
    
    # 2. ë¹„êµ/ë‚˜ì—´ ì§ˆë¬¸ (ì—¬ëŸ¬ ë¬¸ì„œ í•„ìš”)
    comparison_indicators = [
        "ëª¨ë‘", "ì „ë¶€", "ë¦¬ìŠ¤íŠ¸", "ë‚˜ì—´", "ë¹„êµ", "ì°¨ì´", "ì¢…ë¥˜"
    ]
    if any(indicator in query_lower for indicator in comparison_indicators):
        initial_k = 40
        final_k = 15
    
    # 3. ëª©ì°¨/êµ¬ì¡° ì§ˆë¬¸ (íƒ€ê²ŸíŒ…ëœ ê²€ìƒ‰)
    if query_info["type"] == "table_of_contents_lookup":
        initial_k = 10
        final_k = 10
    
    # 4. ìš”ì•½ ì§ˆë¬¸
    if query_info["type"] == "chapter_summary":
        initial_k = 5
        final_k = 5
    
    return {
        "initial_k": initial_k,
        "final_k": final_k,
        "use_broad_search": initial_k > 20
    }

def get_adaptive_weights(query: str, query_info: Dict[str, Any]) -> Tuple[float, float]:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ BM25/Dense ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì • (OCR ê³ ë ¤)"""
    
    query_lower = query.lower()
    
    if '"' in query or "'" in query or 'ã€Œ' in query or 'ã€' in query:
        return (0.3, 0.7)
    
    if len(query.split()) > 5:
        return (0.4, 0.6)
    
    keyword_indicators = [
        "ì´ë€", "ì´ë¼ëŠ”", "ì „ëµ", "ë²•ì¹™", "ì›ë¦¬",
        "íš¨ê³¼", "ë°©ë²•", "ê¸°ë²•", "ì •ì˜"
    ]
    if any(ind in query_lower for ind in keyword_indicators):
        return (0.4, 0.6)
    
    semantic_indicators = [
        "ì™œ", "ì–´ë–»ê²Œ", "ì„¤ëª…", "ì´ìœ ", "ê³¼ì •",
        "ê´€ê³„", "ì˜í–¥", "ì°¨ì´"
    ]
    if any(ind in query_lower for ind in semantic_indicators):
        return (0.3, 0.7)
    
    if query_info.get("type") == "concept_definition":
        return (0.4, 0.6)
    
    return (0.4, 0.6)

HYDE_PROMPT = """You are a helpful assistant. The user will ask a question.
Your task is to write a short, one-paragraph, hypothetical answer to the question.
This answer will be used to find similar documents.
Focus on capturing the key concepts and terminology. Do not say you don't know the answer.
Be concise and clear.

User question: {question}
Hypothetical answer:"""

def generate_hypothetical_answer(query: str, llm: BaseChatModel) -> str:
    """HyDE: LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ê°€ìƒ ë‹µë³€ ìƒì„±"""
    prompt = ChatPromptTemplate.from_template(HYDE_PROMPT)
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"question": query})


def multi_stage_retrieval(
    query_info: Dict[str, Any],
    retrievers: Dict[str, Any],
    all_parent_docs: List[Document],
    original_query: str,
    llm: BaseChatModel,
    embeddings: Embeddings
) -> List[Document]:
    """ë‹¤ë‹¨ê³„ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (HyDE + Reranking)"""
    
    rewritten_query = query_info["rewritten_query"]
    
    # Stage 1: ì ì‘í˜• Kê°’ ë° ê°€ì¤‘ì¹˜ ê²°ì •
    adaptive_k = get_adaptive_k(query_info, rewritten_query)
    bm25_weight, dense_weight = get_adaptive_weights(rewritten_query, query_info)
    
    print(f"\nğŸ” Adaptive Search Config:")
    print(f"   Query Type: {query_info['type']}")
    print(f"   Rewritten Query: {rewritten_query}")
    print(f"   K: {adaptive_k['final_k']} (Initial: {adaptive_k['initial_k']})")
    print(f"   Weights: BM25={bm25_weight:.2f}, Dense={dense_weight:.2f}")

    # Stage 2: HyDE (Hypothetical Document Embeddings)
    hypothetical_answer = generate_hypothetical_answer(rewritten_query, llm)
    print(f"   ğŸ§  HyDE Answer: {hypothetical_answer[:100]}...")
    hyde_embedding = embeddings.embed_query(hypothetical_answer)

    # Stage 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + HyDE-based Dense)
    bm25_retriever = retrievers['bm25']
    bm25_docs = bm25_retriever.get_relevant_documents(rewritten_query)
    
    parent_retriever = retrievers['parent']
    dense_docs_with_scores = parent_retriever.vectorstore.similarity_search_with_score_by_vector(
        hyde_embedding, k=adaptive_k['initial_k']
    )
    
    dense_parent_doc_ids = [doc.metadata['parent_id'] for doc, score in dense_docs_with_scores]
    dense_parent_docs = parent_retriever.docstore.mget(dense_parent_doc_ids)

    # Stage 4: ê²°ê³¼ ë³‘í•© ë° ê°€ì¤‘ì¹˜ ì ìš©
    combined_results = {}
    for i, doc in enumerate(bm25_docs):
        combined_results[doc.page_content] = combined_results.get(doc.page_content, 0) + bm25_weight * (1 / (i + 1))
        
    for i, doc in enumerate(dense_parent_docs):
        if doc:
             combined_results[doc.page_content] = combined_results.get(doc.page_content, 0) + dense_weight * (1 / (i + 1))

    sorted_docs_content = sorted(combined_results.keys(), key=lambda k: combined_results[k], reverse=True)
    
    doc_map = {doc.page_content: doc for doc in all_parent_docs}
    initial_retrieved_docs = [doc_map[content] for content in sorted_docs_content if content in doc_map]

    if not initial_retrieved_docs:
        print("   âš ï¸ No documents retrieved from hybrid search.")
        return []

    print(f"   ğŸ“š Hybrid retrieved (before rerank): {len(initial_retrieved_docs)}")

    # Stage 5: Cross-Encoder Reranking
    global cross_encoder
    if not cross_encoder:
        raise Exception("Cross-encoder model not set. Please call set_cross_encoder_model first.")

    rerank_pairs = [[rewritten_query, doc.page_content] for doc in initial_retrieved_docs[:50]]
    
    if rerank_pairs:
        ce_scores = cross_encoder.predict(rerank_pairs)
    else:
        ce_scores = []
        
    docs_with_scores = []
    for i, (doc, ce_score) in enumerate(zip(initial_retrieved_docs, ce_scores)):
        importance = calculate_chunk_importance(doc, rewritten_query)
        final_score = float(ce_score) + (importance / 100.0)
        doc.metadata['score'] = final_score # Add score to metadata for scorer
        docs_with_scores.append((doc, final_score, importance, ce_score))

    docs_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    final_docs = [doc for doc, _, _, _ in docs_with_scores[:adaptive_k['final_k']]]

    # Stage 6: ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š Top Reranked Results:")
    for i, (doc, score, imp, ce) in enumerate(docs_with_scores[:10], 1):
        page = doc.metadata.get('page', '?')
        sec_hier = doc.metadata.get('section_hierarchy', 'N/A')
        preview = doc.page_content[:60].replace('\n', ' ')
        print(f"   {i:2}. P{page:3} | Score: {score:.3f} (CE: {ce:.3f}, Imp: {imp:.1f}) | Sec: {sec_hier} | {preview}...")

    return final_docs

def get_targeted_documents(
    query_info: Dict[str, Any],
    all_parent_docs: List[Document],
    doc_structure: Dict[str, Any]
) -> List[Document]:
    """ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¼ ì •í™•í•œ ë¬¸ì„œ ì¶”ì¶œ"""
    
    if query_info["type"] == "toc":
        toc_pages = doc_structure.get("toc_pages", [])
        
        if toc_pages:
            print(f"ğŸ¯ ëª©ì°¨ í˜ì´ì§€ ì¶”ì¶œ: {toc_pages}")
            target_docs = [
                doc for doc in all_parent_docs 
                if doc.metadata.get('page', 0) in toc_pages
            ]
            
            if target_docs:
                target_docs.sort(key=lambda d: d.metadata.get('page', 0))
                print(f"  âœ… {len(target_docs)}ê°œ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
                return target_docs
        
        print("âš ï¸ ëª©ì°¨ ìë™ ê°ì§€ ì‹¤íŒ¨, ì´ˆë°˜ 30í˜ì´ì§€ ê²€ìƒ‰")
        target_docs = [
            doc for doc in all_parent_docs 
            if doc.metadata.get('page', 999) <= 30
        ]
        target_docs.sort(key=lambda d: d.metadata.get('page', 0))
        return target_docs
    
    elif query_info["type"] == "chapter_summary":
        ch_num = query_info["chapter_num"]
        chapters = doc_structure.get("chapters", {})
        
        if ch_num in chapters:
            ch_info = chapters[ch_num]
            start, end = ch_info["start_page"], ch_info["end_page"]
            print(f"ğŸ¯ {ch_num}ì¥ í˜ì´ì§€ ë²”ìœ„: {start}-{end}")
            
            target_docs = [
                doc for doc in all_parent_docs
                if start <= doc.metadata.get('page', 0) <= end
            ]
            target_docs.sort(key=lambda d: d.metadata.get('page', 0))
            print(f"  âœ… {len(target_docs)}ê°œ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
            return target_docs
        else:
            print(f"âš ï¸ {ch_num}ì¥ ìœ„ì¹˜ ë¯¸ë°œê²¬")
    
    return []